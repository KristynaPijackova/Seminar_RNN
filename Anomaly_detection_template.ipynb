{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Anomaly_detection_template.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KristynaPijackova/Tutorials_NNs_and_signals/blob/main/Anomaly_detection_template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Anomaly detection with ECG dataset"
      ],
      "metadata": {
        "id": "lrp9P-gZWijt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So in our previous task we used the ECG5000 dataset for a classification task. \n",
        "\n",
        "However, as we noticed when analyzing the dataset, the classes are really imbalanced and thus not suitable for classification task. We tried to improve this by creating a synthetic data with SMOTE method (Synthetic Minority Oversampling TEchnique). While this helped us a little bit and we could still see that the underrepresented classes get mixed up with each other. \n",
        "\n",
        "Due to the great imbalance of the dataset, it is much better to re-consider our problem and instead of classifying signals which are hugely under-represented, we could try learning features from the strongly represented class and do an anomaly detection instead.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Interactive of our example: https://anomagram.fastforwardlabs.com/#/\n"
      ],
      "metadata": {
        "id": "gYJQ7eGcW6nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Few jupyter notebook shortcuts you can use to make your life easier**\n",
        "\n",
        "Run cell: `shift + enter`\n",
        "\n",
        "Undo last action (inside a cell): `ctrl + m + z`\n",
        "\n",
        "Find and replace: `ctrl + h`\n",
        "\n",
        "Insert code cell above: `ctrl + a`\n",
        "\n",
        "Insert code cell below: `ctrl + b`\n",
        "\n",
        "Delete cell: `ctrl + m + d`"
      ],
      "metadata": {
        "id": "nNMzXAoP2irN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's download the data from  http://www.timeseriesclassification.com/Downloads/ECG5000.zip\n",
        "\n",
        "use !wget and !unzip"
      ],
      "metadata": {
        "id": "H5T2AO5cW6yP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amFW0AIpQpcs"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now since we don't need train and test files, we can concatenate the two files together and create one file containing all the data instead.\n",
        "\n",
        "Here we can simply do this by the the unix command `cat`"
      ],
      "metadata": {
        "id": "2-Nk8Re8W2DU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "y8yU2IiTZNI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And we can check if the file-structure still looks the same with !head"
      ],
      "metadata": {
        "id": "3u4HCndxY9wv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jZBEOotmZYiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's import libraries we are about to use"
      ],
      "metadata": {
        "id": "mL5ge9IJZFdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wbWNtDuFYySV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's prepare our data"
      ],
      "metadata": {
        "id": "QcuQvU6fZN9e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once again, we create a dataframe with pandas...\n",
        "\n",
        "`pd.read_csv`"
      ],
      "metadata": {
        "id": "IdRq3rb4ZU_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DtqkASDDZBDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "szXAJk6AZicY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And add prefixes, cause by now we know we cannot work with the column names if they are just numerical...\n",
        "\n",
        "`add_prefix('c')`"
      ],
      "metadata": {
        "id": "RCsR8bxVaqOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JXwMvSI2ahgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So how many samples in each class do we have now? Let's see...\n",
        "\n",
        "`.value_counts()`"
      ],
      "metadata": {
        "id": "KSz0yBYXbCIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rJ-UwBzOa7OZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We split the original data into train and test sets\n",
        "\n",
        "`train_test_split()`"
      ],
      "metadata": {
        "id": "u2YW5IdwZ9HK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "KiBoCZoybP7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now we normalize the data"
      ],
      "metadata": {
        "id": "1L3WKo7QoaL7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use MinMaxScaler that was import from sklearn\n",
        "\n",
        "Here the scaler holds the `preprocessing.MinMaxScaler()` and we fit it on our training data in the second row of the code, which will be applied on the data we want to scale just in a second with `data_scaler.transform(data_we_want_to_scale)`"
      ],
      "metadata": {
        "id": "Wa4BIoHR0QLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sGLySYu4wgBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We take columns 1 to 140 since the 0th column holds the classes and we don't want to change their value."
      ],
      "metadata": {
        "id": "zQS4nSGYDHeE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fYO3rBCtDJU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Separate the data into normal and abnormal sets\n",
        "\n",
        "Well we know we have 5 classes, the first one with index 1 represents normal signals, whilst other 4 represents heart abnormalities.\n",
        "\n",
        "What we want is to have a class 0 that would represent the normal ECG signals and class 1 that holds all the abnormalities. Once again, pandas functions will help us with this. \n",
        "\n",
        "We also don't need the classes - the 0th column, so we take only the columns 1 to 140.\n",
        "\n",
        "`query('c0 == 1').values[:,1:]`"
      ],
      "metadata": {
        "id": "e8CiWFsoceDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ssu-gVHtBc07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's look at our data and the difference"
      ],
      "metadata": {
        "id": "FWWC769Udwss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "K0ZUsQnKdhdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hK11hvRkdnVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the autoencoder"
      ],
      "metadata": {
        "id": "FrfNwgPcd08z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define autoencoder"
      ],
      "metadata": {
        "id": "mNQRVRe37q_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qu4vz98SeN6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile the model"
      ],
      "metadata": {
        "id": "ghZaiE3n7taw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "F4AZtvi2eqHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model"
      ],
      "metadata": {
        "id": "laHxkSaz7vxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "SPLsPtbjf2Kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## So what did our model learn?\n",
        "\n",
        "The autoencoder is made of dense layers is trying to learn the distribution of the signal, so it can reproduce it at it's output. \n",
        "\n",
        "Since we are training it on the normal data it will learn to reproduce the normal data and will struggle with the abnormalities. "
      ],
      "metadata": {
        "id": "KOj9SUCpErM0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We pass the normal test data into the model and plot their distribution in comparisson to their original shape. \n",
        "\n",
        "`model(data).numpy()`"
      ],
      "metadata": {
        "id": "ZRxIger8FUGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MnVZsNaEChzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we do the same thing, but with the abnormal data."
      ],
      "metadata": {
        "id": "BPMIGww0FfGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2KWfnTuHEh1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finding threshold\n",
        "\n",
        "As we can see, the normal data is fitted pretty accurately, whilst the abnormal data does not really copy the original data. \n",
        "\n",
        "And that's what we are focusing at. We can now compute the mean square error `mse` or mean absolute error `mae` of the normal and abnormal data, plot their distribution and based on that decide where the threshold should be, so let's get into it!\n",
        "\n",
        "\n",
        "For predictions you can use `model.predict(data)` and for the `loss tf.keras.losses.mse(reconstructed_data, original_data)`\n",
        "\n"
      ],
      "metadata": {
        "id": "fayvioyTgXLk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Normal train data\n"
      ],
      "metadata": {
        "id": "thMr8TTxIGY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CW3i7bPNgyK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the normal data distribution\n",
        "\n",
        "`plt.hist(loss, bins=50)`"
      ],
      "metadata": {
        "id": "RRXcLY65Gz3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "AWffCdsXG4B2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the mean value and standard deviation of the data\n",
        "\n",
        "`np.mean()` and `np.std()`"
      ],
      "metadata": {
        "id": "TOXyof8AHU1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LmDJQIBChLzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vLCs050shR0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's decide where our threshold is going to be...\n",
        "\n",
        "We can take the mean value + 2*std and see what we get. "
      ],
      "metadata": {
        "id": "kN8KMa85HkEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "lE4POP78ha4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we look at our histogram, 0.064 seem like a good threshold, where most of our normal data lies on the left side. Now we should see, how the distribution of the abnormal data looks like."
      ],
      "metadata": {
        "id": "lVSR5IfnH1LD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "20ufFd_fT7-5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Normal data"
      ],
      "metadata": {
        "id": "N6PEzvUXT-51"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate the loss on the testing data and plot the histogram"
      ],
      "metadata": {
        "id": "aDzDqcnoUaUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CwaYxGAQT-BK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "t2hTXgWKUZLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Abnormal data"
      ],
      "metadata": {
        "id": "ceALq6aYIIyL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, compute the MAE of the predicted abnormal data and display it in the histogram."
      ],
      "metadata": {
        "id": "4HlIzz2qIQxI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MA8CEqlxhf1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "UntiWMOYIOIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seems like our threshold should work well in detecting the most abnormalities. So let's put it all together.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "plt.axvline(threshold, color='r', linewidth=3, linestyle='dashed', label='{:0.3f}'.format(threshold))\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "67lGzpsnIYZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot histograms of normal and abnormal data\n",
        "\n",
        "\n",
        "# plot a vertical line, which displays the threshold\n",
        "\n",
        "\n",
        "# add a legend and title\n",
        "\n"
      ],
      "metadata": {
        "id": "oYdommTvhzcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confusion matrix, ROC curve"
      ],
      "metadata": {
        "id": "JlTD3i5kSz55"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To be able to plot the confusion matrix and ROC curve we first need to count how many errors I. and II. we have -> how many false positives or false negatives. \n",
        "\n",
        "For this we are going to use the tf.math function, where we compare the threshold with the test losses of normal and abnormal data and get an array with True/False values. \n",
        "\n",
        "Next we count how many manu nonzero values we have (non-zero = 1 = True)."
      ],
      "metadata": {
        "id": "_UPXiNGQVDoJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`tf.math.less(loss, threshold)`\n",
        "`tf.math.greater(loss, threshold)`"
      ],
      "metadata": {
        "id": "fUyKW-z480h-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fDoD58YO5jxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`tf.math.count_nonzero`"
      ],
      "metadata": {
        "id": "r-JO0JBx8_1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1T4BoheU5pMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "g28zUtz554rH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To **plot the confusion matrix**, we need to create a list which will hold the values we want to display. \n",
        "\n",
        "We can also define the categories to display as ticks - normal/anomalities. \n",
        "\n",
        "And to plot the data we will use **seaborn**. Seaborn is a library for statistical data visualization and is based on matplotlib, but is more user friendly.\n",
        "Basicly all we could do is to write `sns.heatmap(cm)` and we would have our confusion matrix. However we added few extras to make it nice and representable. "
      ],
      "metadata": {
        "id": "-57kJkMjgECx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "lgIdL1Mk5w1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we combine the first three cells of this block into one as a function which will predict true positives, false negatives, true negatives and false positives. We will need this function to get us values, so we can plot ROC."
      ],
      "metadata": {
        "id": "EncIAGvavseV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mLePzSefjk_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate the roc values - tpr and fpr...\n",
        "\n",
        "...first define empty list which will hold the values\n",
        "\n",
        "...then create a for loop which will take 100 values from 0 to 1 (use `np.linspace(from, to, steps)`)\n",
        "\n",
        "...now use the defined predictor function and, where the threshold is the value 0 to 1 of the forloop\n",
        "\n",
        "...calculate the trp and frp for each iteration `tpr = tp/(tp + fn)` and `fpr = fp/(fp + tn)`\n",
        "\n",
        "...`append` the tpr and fpr values in the roc list"
      ],
      "metadata": {
        "id": "i_oFJtI5GoI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5-vvgy4F58W_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the ROC curve"
      ],
      "metadata": {
        "id": "CUGlz5TuyRU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hJfJ6ThdhigQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}